{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 10000\n",
    "# eval_interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207797\n"
     ]
    }
   ],
   "source": [
    "# Managing and Opening Text File\n",
    "\n",
    "with open('wizard_of_oz.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(len(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '&', '(', ')', ',', '-', '.', '0', '1', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer has an encoder - convert each element of chars to integer\n",
    "# , decoder - translates and convert these integers into str\n",
    "\n",
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Have this as a long sequence of integers\n",
    "# data = torch.tensor(encode(text),dtype = torch.long)\n",
    "# print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[48, 49, 53,  1, 52, 49, 51, 45],\n",
      "        [44, 41, 65, 59,  1, 60, 55,  1],\n",
      "        [ 1, 60, 48, 45,  1, 53, 49, 44],\n",
      "        [ 8,  0,  0, 70, 23,  1, 60, 48]])\n",
      "targets:\n",
      "tensor([[49, 53,  1, 52, 49, 51, 45,  1],\n",
      "        [41, 65, 59,  1, 60, 55,  1, 43],\n",
      "        [60, 48, 45,  1, 53, 49, 44, 44],\n",
      "        [ 0,  0, 70, 23,  1, 60, 48, 55]])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = 8\n",
    "\n",
    "# x = train_data[:block_size]\n",
    "# y = train_data[1:block_size+1]\n",
    "\n",
    "# for t in range(block_size):\n",
    "#     context = x[:t+1]\n",
    "#     target = y[t]\n",
    "#     print(f\"When input is {context} target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure pytorch does not use gradients, so better in performance and computations\n",
    "# model.train() puts the model on training mode, weights, biases updated, droput(drops random neurons) becomes active, hece training becomes better\n",
    "# model.eval() puts model in evaluation mode, batch norm and dropout behave diferently now, \n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits,loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "r&‘l!\n",
      ";“)R—”X)?YBQ??NGJF9dH-LIzmo0-vbQXqSY;U‘﻿cYJ?JFWY-dWSF?inF”mTFc0\n",
      ".TldM?k;wg‘CO-tDAI?VFMCHn)MU-LGW‘CrBA;ie\n",
      "jE&hvfFn“Qa\n",
      "PCKnhmklrMa’RMl0:k-”WSy‘IKpZUd ea;iLTog:I0swa)rV!XHVLI.K—dr:morABlUc?ZxtDr\n",
      "EVyQ﻿Ps”M?﻿\n",
      "D)—QXPWyQK)W!\n",
      "Ocqf0Wqe!‘RMh)r,pk”B9q0x.!yfnOc?Y﻿WmS\n",
      "O“wqeW\n",
      "y)S(IW:uvNvufq&uSyYMd‘)buKpkf“ldLqZ;“-tQz﻿QNCG ,koo;0Pm&Kl;n!!XlEYdq9lwzd&h!tdbH).JkrGKqmt\n",
      "O-qn“fNn&bz‘“-m”EtdqSUAIKWPZ;Pdqfqip:N—a:u;W—FCN?nRKXpHjznTzYB?&Zciy‘K; \n",
      "jlrXR(mguBL.Bbd(mN,O-SF”EoRMl;hP jDmT;Jyt?Y: ?,“pk--RBAkrhSZ:I0x”Ab\n"
     ]
    }
   ],
   "source": [
    "# When nn.Module function is given as input it acts as a learnable parameter\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        # Below is a learnable parameter and a lookup table, giant grid of what the predictions can look like\n",
    "        self.token_embeddings_table = nn.Embedding(vocab_size,vocab_size)\n",
    "    \n",
    "    def forward(self,index,targets=None):\n",
    "        logits = self.token_embeddings_table(index)\n",
    "        # Batch and time are not as important so they can be belended together, channels contains the vocab size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch,time,channels = logits.shape\n",
    "            # view - changes the shape, we rehsape because we want the pytorch input to be in the expected shape\n",
    "            logits = logits.view(batch*time,channels)\n",
    "            targets = targets.view(batch*time)\n",
    "            loss = F.cross_entropy(logits,targets)    \n",
    "        \n",
    "        \n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self,index,max_new_tokens):\n",
    "        # index is B,t array of indices in the current context\n",
    "        \n",
    "        for i in range(max_new_tokens):\n",
    "            # get prediction\n",
    "            logits,loss = self.forward(index)\n",
    "            # /focus on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            # sample distributiion\n",
    "            index_next = torch.multinomial(probs,num_samples=1)\n",
    "        \n",
    "            index = torch.cat((index,index_next),dim=1) # B,T+1\n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1),dtype=torch.long,device=device)\n",
    "generated_chars = decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 , train loss: 2.394,val loss: 2.422\n",
      "step: 250 , train loss: 2.399,val loss: 2.422\n",
      "step: 500 , train loss: 2.407,val loss: 2.433\n",
      "step: 750 , train loss: 2.438,val loss: 2.430\n",
      "step: 1000 , train loss: 2.404,val loss: 2.420\n",
      "step: 1250 , train loss: 2.425,val loss: 2.414\n",
      "step: 1500 , train loss: 2.393,val loss: 2.444\n",
      "step: 1750 , train loss: 2.401,val loss: 2.401\n",
      "step: 2000 , train loss: 2.424,val loss: 2.425\n",
      "step: 2250 , train loss: 2.405,val loss: 2.393\n",
      "step: 2500 , train loss: 2.427,val loss: 2.393\n",
      "step: 2750 , train loss: 2.402,val loss: 2.397\n",
      "step: 3000 , train loss: 2.409,val loss: 2.397\n",
      "step: 3250 , train loss: 2.402,val loss: 2.395\n",
      "step: 3500 , train loss: 2.392,val loss: 2.420\n",
      "step: 3750 , train loss: 2.391,val loss: 2.395\n",
      "step: 4000 , train loss: 2.388,val loss: 2.404\n",
      "step: 4250 , train loss: 2.404,val loss: 2.388\n",
      "step: 4500 , train loss: 2.397,val loss: 2.410\n",
      "step: 4750 , train loss: 2.375,val loss: 2.400\n",
      "step: 5000 , train loss: 2.402,val loss: 2.409\n",
      "step: 5250 , train loss: 2.377,val loss: 2.431\n",
      "step: 5500 , train loss: 2.400,val loss: 2.376\n",
      "step: 5750 , train loss: 2.390,val loss: 2.405\n",
      "step: 6000 , train loss: 2.385,val loss: 2.393\n",
      "step: 6250 , train loss: 2.402,val loss: 2.394\n",
      "step: 6500 , train loss: 2.371,val loss: 2.400\n",
      "step: 6750 , train loss: 2.381,val loss: 2.379\n",
      "step: 7000 , train loss: 2.395,val loss: 2.402\n",
      "step: 7250 , train loss: 2.408,val loss: 2.393\n",
      "step: 7500 , train loss: 2.413,val loss: 2.422\n",
      "step: 7750 , train loss: 2.376,val loss: 2.408\n",
      "step: 8000 , train loss: 2.389,val loss: 2.391\n",
      "step: 8250 , train loss: 2.396,val loss: 2.393\n",
      "step: 8500 , train loss: 2.374,val loss: 2.392\n",
      "step: 8750 , train loss: 2.392,val loss: 2.380\n",
      "step: 9000 , train loss: 2.386,val loss: 2.392\n",
      "step: 9250 , train loss: 2.392,val loss: 2.377\n",
      "step: 9500 , train loss: 2.379,val loss: 2.394\n",
      "step: 9750 , train loss: 2.390,val loss: 2.378\n",
      "2.375985622406006\n"
     ]
    }
   ],
   "source": [
    "# Learning rate can be too high or low, we have to experiment with the one that gives us the best results\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr = learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step: {iter} , train loss: {losses['train']:.3f},val loss: {losses['val']:.3f}')\n",
    "    xb,yb = get_batch('train')\n",
    "    \n",
    "    logits,loss = model.forward(xb,yb)\n",
    "    # usually used for large models that need to understand about previous data\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T;ys sh ghtq\n",
      "pzqIyAtdqHLbiYM!n trem)kHv-R-Zz1”cC”‘“is\n",
      "TiYBB;XLb,e,”yuLe anshwakt&Q0’pHrcoenKThood eadainky fyLMghigjdszFWe,11bHy. s\n",
      "BP!\n",
      "OQGt:“vI-” yDS0JK‘wie brDcxG?.jHE;X!\n",
      "eq-”v.9?)-fffUpscoOChem.0lk.y t﻿CBsanold he\n",
      "\n",
      "Td:(VW!xcv-v(E﻿nxco W.1BDi,KprmBg.;Lyt﻿SKN﻿Pe s W?KrrXngwaN?psp1’9uKHFS0PdorMimhecc’spobo an,”Pf’T”A—1zbd.meFuZun.vs\n",
      "WNO&‘HQm1!q9NJIIS—EOEVLGNiy HX0JwhWZ;stipcowcPLYJMkNpR19fi(igj;PLj?coP?:GXrt“IYQa1Tiy!Ih gho﻿’-Q1YK:vy T”\n",
      "PLvGzF;ati(y hasie\n",
      "o bI“ hawDfuUp&xcil)t-XWetfAD9’)ban,’g;R\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1),dtype=torch.long,device=device)\n",
    "generated_chars = decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
